{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from statistics import stdev\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_chunk = 300\n",
    "num_chunks_per_slice = 65\n",
    "num_chunks_per_beat = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadbeatmap(beatmap, num_beats, num_chunks_per_beat=8):\n",
    "    if beatmap[len(beatmap)-5:len(beatmap)] != \".json\":\n",
    "        print(\"Beatmap file \" + audio + \" is not of type .json\")\n",
    "        return -1\n",
    "    \n",
    "    with open(beatmap) as f:\n",
    "        data = json.load(f)\n",
    "  \n",
    "    notes = \"_notes\"\n",
    "    time = \"_time\"\n",
    "    line_index = \"_lineIndex\" #column number\n",
    "    line_layer = \"_lineLayer\" #row number\n",
    "    note_color = \"_type\" #0 is one color and 1 is the other\n",
    "    cut_direction = \"_cutDirection\"#9 cut directions\n",
    "\n",
    "    dim_0 = num_beats * num_chunks_per_beat\n",
    "    \n",
    "    # number of rows and columns in the playfield\n",
    "    # number of cells in the playfield (each cell can hold at most 1 note)\n",
    "    playfield_rows = 3\n",
    "    playfield_cols = 4\n",
    "    playfield_cell_count = playfield_rows * playfield_cols\n",
    "    \n",
    "    # number of colors (2): red, blue (order unknown)\n",
    "    # number of directions notes can face (9): \n",
    "    # up, down, left, right, up-left, up-right, down-left, down-right, dot (order unknown)\n",
    "    note_color_count = 2\n",
    "    note_direction_count = 9\n",
    "    \n",
    "    # dimensions for a 'one-hot' representation of a single time unit (chunk)\n",
    "    dim_1 = playfield_rows\n",
    "    dim_2 = playfield_cols\n",
    "    dim_3 = (note_color_count + 1) + note_direction_count\n",
    "    \n",
    "    # initialize matrix to zeros, then set the \"no note\" bit for each block at each timestep to 1\n",
    "    outMatrix = np.zeros(shape=(dim_0, dim_1, dim_2, dim_3))\n",
    "    outMatrix[:,:,:,0] = 1\n",
    "    \n",
    "\n",
    "    # for every note in the beatmap, set the color and direction bits for the proper cell to 1\n",
    "    for n in range(len(data[notes])):\n",
    "        entry = int(np.round(data[notes][n][time]*num_chunks_per_beat)) #convert time to row index by rounding to nearest 1/8 beat\n",
    "        if data[notes][n][note_color] < 2:\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [data[notes][n][note_color]+1] = 1\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [0] = 0\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [data[notes][n][cut_direction]+3] = 1\n",
    "\n",
    "    return outMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(x): \n",
    "    return (x - np.apply_along_axis(np.mean, 0, x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadsong(audio, samples_per_chunk=300, num_chunks_per_slice=65, num_chunks_per_beat=8, verbose=0):\n",
    "    if audio[len(audio)-4:len(audio)] != \".ogg\":\n",
    "        print(\"Audio file \" + audio + \" is not of type .ogg\")\n",
    "        return -1\n",
    "    \n",
    "    y, sr = librosa.load(audio)\n",
    "    \n",
    "    song_length = librosa.get_duration(y=y,sr=sr) / 60.0\n",
    "    tempo = np.round(librosa.beat.tempo(y, sr=sr))\n",
    "    new_sample_rate = (tempo/200)*8000\n",
    "    \n",
    "    y = librosa.resample(y, sr, new_sample_rate)\n",
    "    \n",
    "    number_of_beats = int(tempo * song_length)\n",
    "    \n",
    "    return y[0:(len(y)//(samples_per_chunk*num_chunks_per_beat)*(samples_per_chunk*num_chunks_per_beat))], new_sample_rate, number_of_beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_song(song, sample_per_chunk = 300):\n",
    "    song_y = song.reshape(len(song)//300,300)\n",
    "    song_fft = np.abs(np.apply_along_axis(np.fft.fft, 1, song_y))[:,0:(int)(samples_per_chunk/2)+1]\n",
    "    song_fft_mc = np.apply_along_axis(mean_center, 0, song_fft)\n",
    "    return song_fft_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_song(init_song, init_beatmap, song_filepath, beatmap_filepath, num_chunks_per_beat = 8):\n",
    "    loaded_song_y, loaded_song_sr, num_beats = loadsong(song_filepath)\n",
    "    \n",
    "    prepped_song = prep_song(loaded_song_y)\n",
    "    \n",
    "    loaded_beatmap = loadbeatmap(beatmap_filepath, num_beats)\n",
    "    \n",
    "    if init_song == None and init_beatmap == None:\n",
    "        init_song = []\n",
    "        init_beatmap = []\n",
    "        \n",
    "    for i in range(num_beats*num_chunks_per_beat-256):\n",
    "        init_song.append(prepped_song[i:i+256]) \n",
    "    for i in range(num_beats*num_chunks_per_beat-256):\n",
    "        init_beatmap.append(loaded_beatmap[i:i+256]) \n",
    "    \n",
    "    init_song = np.array(init_song)\n",
    "    init_beatmap = np.array(init_beatmap)\n",
    "        \n",
    "    return init_song, init_beatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(song_list, beatmap_list):\n",
    "    X, Y = append_song(None, None, song_list[0], beatmap_list[0])\n",
    "    for x, y in zip(song_list[1:], beatmap_list[1:]):\n",
    "        X, Y = append_song(X, Y, x, y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/scipy/fftpack/basic.py:160: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  z[index] = x\n"
     ]
    }
   ],
   "source": [
    "X, Y  = data_prep([\"song.ogg\"], [\"Expert.json\"]) #\"song.ogg\"], [\"Expert.json\", \"Expert.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3856, 256, 151)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/scipy/fftpack/basic.py:160: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  z[index] = x\n"
     ]
    }
   ],
   "source": [
    "song, song_sr, numBeats = loadsong(\"song.ogg\")\n",
    "song = prep_song(song)\n",
    "beatmap = loadbeatmap(\"Expert.json\", numBeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numBeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = np.empty\n",
    "beatmaps = np.empty\n",
    "songpaths = np.empty\n",
    "mappaths = np.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = np.append(songs, song)\n",
    "beatmaps = np.append(beatmaps, beatmap)\n",
    "songpaths = np.append(songpaths, \"song.ogg\")\n",
    "mappaths = np.append(mappaths, \"Expert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data_prep() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-429dfb1ad3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepped_songs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepped_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeatmaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msongpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data_prep() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "prepped_songs, prepped_maps = data_prep(songs, beatmaps, songpaths, mappaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
