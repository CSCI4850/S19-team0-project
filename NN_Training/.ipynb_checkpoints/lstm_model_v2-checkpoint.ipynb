{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing relevant modules\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot   as plt\n",
    "import librosa\n",
    "import numpy as np\n",
    "import json\n",
    "from statistics import stdev\n",
    "\n",
    "from pandas                import read_csv, to_datetime\n",
    "from numpy                 import reshape, array\n",
    "from datetime              import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers          import Dense, Dropout, LSTM, Input, TimeDistributed\n",
    "from keras.models          import Model\n",
    "from keras_tqdm            import TQDMNotebookCallback\n",
    "from IPython.display       import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks       import EarlyStopping\n",
    "from keras.layers import Conv2DTranspose\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_chunk = 300\n",
    "num_chunks_per_slice = 65\n",
    "num_chunks_per_beat = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadbeatmap(beatmap, num_beats, num_chunks_per_beat=8):\n",
    "    if beatmap[len(beatmap)-5:len(beatmap)] != \".json\":\n",
    "        print(\"Beatmap file \" + audio + \" is not of type .json\")\n",
    "        return -1\n",
    "    \n",
    "    with open(beatmap) as f:\n",
    "        data = json.load(f)\n",
    "  \n",
    "    notes = \"_notes\"\n",
    "    time = \"_time\"\n",
    "    line_index = \"_lineIndex\" #column number\n",
    "    line_layer = \"_lineLayer\" #row number\n",
    "    note_color = \"_type\" #0 is one color and 1 is the other\n",
    "    cut_direction = \"_cutDirection\"#9 cut directions\n",
    "\n",
    "    dim_0 = num_beats * num_chunks_per_beat\n",
    "    \n",
    "    # number of rows and columns in the playfield\n",
    "    # number of cells in the playfield (each cell can hold at most 1 note)\n",
    "    playfield_rows = 3\n",
    "    playfield_cols = 4\n",
    "    playfield_cell_count = playfield_rows * playfield_cols\n",
    "    \n",
    "    # number of colors (2): red, blue (order unknown)\n",
    "    # number of directions notes can face (9): \n",
    "    # up, down, left, right, up-left, up-right, down-left, down-right, dot (order unknown)\n",
    "    note_color_count = 2\n",
    "    note_direction_count = 9\n",
    "    \n",
    "    # dimensions for a 'one-hot' representation of a single time unit (chunk)\n",
    "    dim_1 = playfield_rows\n",
    "    dim_2 = playfield_cols\n",
    "    dim_3 = (note_color_count + 1) + note_direction_count\n",
    "    \n",
    "    # initialize matrix to zeros, then set the \"no note\" bit for each block at each timestep to 1\n",
    "    outMatrix = np.zeros(shape=(dim_0, dim_1, dim_2, dim_3))\n",
    "    outMatrix[:,:,:,0] = 1\n",
    "    \n",
    "\n",
    "    # for every note in the beatmap, set the color and direction bits for the proper cell to 1\n",
    "    for n in range(len(data[notes])):\n",
    "        entry = int(np.round(data[notes][n][time]*num_chunks_per_beat)) #convert time to row index by rounding to nearest 1/8 beat\n",
    "        if data[notes][n][note_color] < 2:\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [data[notes][n][note_color]+1] = 1\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [0] = 0\n",
    "            outMatrix[entry] \\\n",
    "                     [data[notes][n][line_layer]] \\\n",
    "                     [data[notes][n][line_index]] \\\n",
    "                     [data[notes][n][cut_direction]+3] = 1\n",
    "\n",
    "    return outMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center(x): \n",
    "    return (x - np.apply_along_axis(np.mean, 0, x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadsong(audio, samples_per_chunk=300, num_chunks_per_slice=65, num_chunks_per_beat=8, verbose=0):\n",
    "    if audio[len(audio)-4:len(audio)] != \".ogg\":\n",
    "        print(\"Audio file \" + audio + \" is not of type .ogg\")\n",
    "        return -1\n",
    "    \n",
    "    y, sr = librosa.load(audio)\n",
    "    \n",
    "    song_length = librosa.get_duration(y=y,sr=sr) / 60.0\n",
    "    tempo = np.round(librosa.beat.tempo(y, sr=sr))\n",
    "    new_sample_rate = (tempo/200)*8000\n",
    "    \n",
    "    y = librosa.resample(y, sr, new_sample_rate)\n",
    "    \n",
    "    number_of_beats = int(tempo * song_length)\n",
    "    \n",
    "    return y[0:(len(y)//(samples_per_chunk*num_chunks_per_beat)*(samples_per_chunk*num_chunks_per_beat))], new_sample_rate, number_of_beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_song(song, sample_per_chunk = 300):\n",
    "    song_y = song.reshape(len(song)//300,300)\n",
    "    song_fft = np.abs(np.apply_along_axis(np.fft.fft, 1, song_y))[:,0:(int)(samples_per_chunk/2)+1]\n",
    "    song_fft_mc = np.apply_along_axis(mean_center, 0, song_fft)\n",
    "    return song_fft_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_song(init_song, init_beatmap, song_filepath, beatmap_filepath, num_chunks_per_beat = 8):\n",
    "    loaded_song_y, loaded_song_sr, num_beats = loadsong(song_filepath)\n",
    "    \n",
    "    prepped_song = prep_song(loaded_song_y)\n",
    "    \n",
    "    loaded_beatmap = loadbeatmap(beatmap_filepath, num_beats)\n",
    "    \n",
    "    if init_song == None and init_beatmap == None:\n",
    "        init_song = []\n",
    "        init_beatmap = []\n",
    "        \n",
    "    for i in range(num_beats*num_chunks_per_beat-256):\n",
    "        init_song.append(prepped_song[i:i+256]) \n",
    "    for i in range(num_beats*num_chunks_per_beat-256):\n",
    "        init_beatmap.append(loaded_beatmap[i:i+256]) \n",
    "    \n",
    "    init_song = np.array(init_song)\n",
    "    init_beatmap = np.array(init_beatmap)\n",
    "        \n",
    "    return init_song, init_beatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(song_list, beatmap_list):\n",
    "    X, Y = append_song(None, None, song_list[0], beatmap_list[0])\n",
    "    for x, y in zip(song_list[1:], beatmap_list[1:]):\n",
    "        X, Y = append_song(X, Y, x, y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/scipy/fftpack/basic.py:160: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  z[index] = x\n"
     ]
    }
   ],
   "source": [
    "X, Y  = data_prep([\"song.ogg\"], [\"Expert.json\"]) #\"song.ogg\"], [\"Expert.json\", \"Expert.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3856, 256, 151)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1DTranspose(input_tensor, filters, kernel_size, strides=2, padding='same'):\n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding)(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3856, 256, 3, 4, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Model\n",
    "\n",
    "#lstm_size = 100\n",
    "#input_layer = keras.layers.Input(shape=(X.shape[1], X.shape[2]))\n",
    "cnn_layer = keras.layers.Conv1D(32, kernel_size=((X.shape[1] - 143)), \n",
    "                                activation='relu',\n",
    "                                input_shape=[X.shape[1], X.shape[2]])\n",
    "lstm_layer = keras.layers.LSTM(lstm_size, return_sequences = True)\n",
    "dense_direction = keras.layers.Dense(9, activation ='softmax')\n",
    "dense_color = keras.layers.Dense(3, activation = 'softmax')\n",
    "new_dense = keras.layers.Dense(14400, activation = 'relu')\n",
    "reshaper = keras.layers.Reshape((3, 4, 12))\n",
    "#new_dense = keras.layers.Dense(NUM_ELEMENTS_NEEDED_IN_OUTPUT, activation='relu')\n",
    "#reshaper =keras.layers.Reshape(HERE YOU RESHAPE THE PREV NUM_ELEMENTS_NEEDED_IN_OUTPUT TO BE THE TENSOR DIMS YOU NEED)\n",
    "\n",
    "\n",
    "#cnn_output = cnn_layer(input_layer)\n",
    "lstm_output = lstm_layer(cnn_output)\n",
    "\n",
    "deconv = keras.layers.UpSampling1D(2)(lstm_output)\n",
    "deconv = keras.layers.Conv1D(32, 3, padding='same')(deconv)\n",
    "\n",
    "\n",
    "test_tensor = Input(shape= (144,))\n",
    "#reshape = keras.layers.Reshape((3, 4, 12))(lstm_output)\n",
    "\n",
    "direction_output = dense_direction(lstm_output)\n",
    "color_output = dense_color(lstm_output)\n",
    "dense_inputs = [direction_output, color_output]\n",
    "final_output = keras.layers.concatenate(dense_inputs, axis = -1)\n",
    "new_dense_output = new_dense(final_output)\n",
    "#reshape = keras.layers.Reshape((3, 4, 12))(final_output)\n",
    "\n",
    "#new_model = keras.Model(input_layer, final_output)\n",
    "new_model.compile(loss = keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "new_model.summary()\n",
    "#SVG(model_to_dot(new_model).create(prog='dot',format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.eval of <tf.Tensor 'lstm_93/transpose_1:0' shape=(?, ?, 100) dtype=float32>>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected concatenate_57 to have 3 dimensions, but got array with shape (3856, 256, 3, 4, 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-99ba079144f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                    callbacks=[TQDMNotebookCallback()])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfshome/apps/python-3.6.7/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected concatenate_57 to have 3 dimensions, but got array with shape (3856, 256, 3, 4, 12)"
     ]
    }
   ],
   "source": [
    "batch_size = 225\n",
    "epochs = 300\n",
    "history = model.fit(X, Y,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs=epochs,\n",
    "                   verbose=0,\n",
    "                   callbacks=[TQDMNotebookCallback()])\n",
    "print('Accuracy:', model.evaluate(X,Y)[1]*100.0,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
