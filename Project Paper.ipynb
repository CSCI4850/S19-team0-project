{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Automatically Generating Beat-maps using Neural Networks\\\\}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Thai Do}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, Tennessee USA \\\\\n",
    "tmd4w@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Jackson Goble}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, Tennessee USA \\\\\n",
    "jlg2av@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} Nzubechukwu Molokwu}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, Tennessee USA \\\\\n",
    "num2a@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{4\\textsuperscript{rd} Levi Shirley}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, Tennessee USA \\\\\n",
    "cls6a@mtmail.mtsu.edu}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "Beat Saber allows for anybody to create a beat-map for any song they wish to, but not all of these maps can be considered as fun and playable. There are many maps for more obscure songs that are tuned for very high difficulties. Due to the complexity of both beat-maps and songs, little work has been done to automate the process of generating a beat-map. It is hoped that by exposing a network to many songs and their corresponding beat-maps, it would be capable of generalizing these associated values and generating new beat-maps given a song.  We experimented with two different neural networks to automatically generate beat-maps, one using a Long Short Term Memory layer, and the second using only Convolutional layers.  Our networks are currently capable of producing a similar beat-map given a song and its corresponding beat-map.  Because of these results, we can conclude that our current methology is heading in the right direction of producing beat-maps for any song, but we currently do not have an actual network that is capable of this.\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks, Long Short-Term Memory, Rectified Linear Unit, Fourier Transformation, Beat-map, CNN, RNN, LSTM, RELU, Python, Keras\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "\n",
    "Rhythm games have been around for a long time. Games such as Dance Dance Revolution and Guitar Hero are some of the most well-known rhythm game titles. Recently, with the rapid growth of virtual reality, a new popular rhythm game called Beat Saber has emerged. In most rhythm games, developers hand-make levels called beat-maps that correspond to the feel and characteristics of specific songs. Beat Saber, is no exception. However, avid fans of the game have created tools that allow anyone to create and play their own beat-maps for any song they wish. These beat-maps are often shared online so that other players can enjoy playing them as well. Unfortunately, because these beat-maps can be made and uploaded by anyone with any level of experience, the quality of the beat-map is dependent on the person that created the map. Some user-developed beat-maps are either impossible to play, or aren't consistent with the characteristics of the song. \\par Because of the inconsistency among user-developed beat-maps, our goal is to develop and train a neural network that is capable of automatically generating a fun and playable beat-map given any song. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "\n",
    "In Beat Saber, a beat-map is stored as a JSON file containing information (e.g. time, location) about the various blocks that the player is supposed to hit (notes), avoid hitting (bombs), or dodge with their body (walls). The game itself is played on a 3x4 grid, providing 12 areas that these blocks can be placed on. A note block has two main properties: its color and direction. The color determines which hand it the player should use to hit the block, and the direction determines the direction in which the player should swing his arm to hit it. A note can either be red or blue, and there are nine possible directions: the 8 cardinal directions or a dot for 'any' direction. In addition to beat-maps, the songs themselves are also complex structures. Songs contain characteristics such as rhythm, tempo, and frequency \\cite{tempo05}. Because each song has varying characteristics, it is difficult for an automatic generator to determine what type of object should appear accordingly. \\par Even though walls and bombs are part of the game, we feel that the note blocks are the main feature of the game that either make or break a beat-map. Therefore, even though the best generator would be able to place notes, bombs, and walls, we decided that we should ignore bombs and walls, and focus only on the placement of notes. By removing the possibility of bombs and walls, we reduce the complexity of the beat-map we generate and can focus on the main attraction of the game.  \\par We experimented with two different neural networks to see which would yield better results.  For the first neural network, we decided to use both a Convolutional Neural Network (CNN), and a Long Short Term Memory (LSTM), a variation of a Recurrent Neural Network (RNN), to predict whether a note exists or not at a moment in the song and if it does, determine the color and direction of the note. There is a similar project on Github called Osumapper that was built for a rhythm game known as \"osu!\" \\cite{kotritrona18}. Though the games are different, the concepts of classifying and placing the notes share a lot of similarities. This project has a similar network to our first network. Osumapper first inputs a song into a CNN. After getting an output from the CNN, it then puts the output into a LSTM to classify the type of note that goes with the song. The biggest difference between our first network and Osumapper is that Osumapper feeds its constructed map into a generative adversarial network (GAN), something that our network doesn't include. \\par Our second network consists of only convolutional and deconvolutional layers. The main difference of this network compared to the second network is that it lacks a LSTM layer. This network lets the CNN layers convolving over the data handle the time dimensional aspect.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "% DATA SUBSECTION\n",
    "\\subsection{Data}\n",
    "% TRAINING DATA SUBSUBSECTION\n",
    "\\subsubsection{Gathering Training Data}\n",
    "One of the most important aspects of training any neural network is gathering the appropiate training data. Therefore, the first step was to find a source that contains beat-maps that users have developed. For our project, we used the website www.beatsaver.com. This website allows users to upload, share, and rate each others beat-map creations. Because there are so many beat-maps generated by different people, we needed to come up with a set of requirements that a beat-map must pass in order to be used in our training set. The first requirement to be considered an appropiate beat-map is that it needs to be highly-rated by the playerbase of Beat Saber. A map that is highly-rated means that it is well-developed and enjoyed by a majority of the players who have played it. Secondly, the beat-map level must be categorized as an 'expert' difficulty map. The reason for this is is twofold. First, 'expert' difficulty is commonly considered to offer the proper Beat Saber experience. Second, there is a good amount of variation in them that we think would be interesting to have a neural net attempt to learn, as opposed to easier difficulties that contain a lot of repetition and note blocks that appear only on the main beat. With these requirements set, we were able to gather a number of \"fun and playable\" beat-maps.\n",
    "% PREPARING INPUT SONG SUBSUBSECTION\n",
    "\\subsubsection{Preparing the Input Song}\n",
    "\\begin{figure}[htpb]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth,height=6cm,keepaspectratio=true]{Fourier_unit_pulse.png}\n",
    "    \\caption{\n",
    "        Example of a Fourier Transformation, from Wikipedia, the free encyclopedia \\cite{fourier}. On the left of this figure is the data before Fourier Transformation. On the right side of this figure is the data after the Fourier Transformation. \n",
    "    }\n",
    "    \\label{fig:Fourier Transformation}\n",
    "\\end{figure}\n",
    "Our project works with \".ogg\" audio files, so we needed a way to load and manipulate such files. For this, we used the Librosa Python library \\cite{librosa15}. With Librosa, we are able to load the audio file, and find the duration of the file in seconds. In addition to that, it also allows us an easy way to both resample the song and determine the tempo.  After resampling the song to a specific number of samples per beat, we divide the audio file into 1/8th notes and perform a Fourier transformation on each of them. This makes the file easier for a neural network to learn from because the Fourier transformation breaks down the audio into simpler constituent frequencies. We then take the absolute value of the Fourier transformation to remove the imaginary component and extract a value that represents the amplitude of each these constituent frequencies. We then mean-center this transformed data to help the neural net base its decisions on the variation in the frequencies rather than the amplitude. Figure 1 shows an example of a Fourier transformation on a sample audio file.\n",
    "% FIRST NEURAL NETWORK SUBSECTION\n",
    "\\subsection{The First Neural Network}\n",
    "% STRUCTURE OF FIRST NEURAL NETWORK\n",
    "\\begin{figure}[htpb]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth,height=6cm,keepaspectratio=true]{initial_model_architecture.png}\n",
    "    \\caption{\n",
    "        A visual representation of the first neural network that we built. \n",
    "    }\n",
    "    \\label{fig:First Neural Network}\n",
    "\\end{figure}\n",
    "For our first neural network, we started with a convolutional layer that takes in the input data, and performs a convolution on it allowing us to access a larger time interval. For the activation function of our convolutional layer, we chose to use the default rectified linear unit (RELU) function. This function is typically the default function used for CNNs. After the convolutional layer, the data will then go into the LSTM layer. The LSTM layer looks at each particular piece of the song while keeping in mind the previous piece and the upcoming piece in order to update its weights appropiately. The end goal of this layer is to generate two outputs, the direction and color of the note. Once the two outputs are produced, they each go into a dense layer. Because these layers are just a simple classification layer, we used the softmax activation function. Finally, the two outputs are concatenated together to form the notes that are placed in the beat-map.  Figure 2 shows a visual representation of the first network. \n",
    "% SECOND NEURAL NETWORK SUBSECTION\n",
    "\\subsection{The Second Neural Network}\n",
    "\\begin{figure}[htpb]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth,height=6cm,keepaspectratio=true]{secondNetwork.png}\n",
    "    \\caption{\n",
    "        A visual representation of part of the second neural network that we built. \n",
    "    }\n",
    "    \\label{fig:First Neural Network Training Result}\n",
    "\\end{figure}\n",
    "Our second neural network is relatively large due to the high number of convoultional and deconvolutional layers in the network. This network handles the time dimension aspect by using several CNN layers to funnel the data and flatten the data into a dense layer. This data is then deconvolved into the necessary output shape. Similar to the first neural network, it is split apart into two separate dense layers corresponding to color and direction. Finally, it is concatenated together to form the notes that will go into beat map.  Figure 3 shows a visual representation of a part of the second network."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "% TRAINING FIRST NEURAL NETWORK\n",
    "\\subsection{First Neural Network Result}\n",
    "\\begin{figure}[htpb]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth,height=6cm,keepaspectratio=true]{cnn_lstm_model_graph.png}\n",
    "    \\caption{\n",
    "        The training and testing results of our first neural network.\n",
    "    }\n",
    "    \\label{fig:First Neural Network Training Result}\n",
    "\\end{figure}\n",
    "To train this neural network, we pass it a song that has been transformed and broken up into separate chunks as described above, as well as its corresponding beat-map for the same moments in time. After it finishes training on the data, it validates the newly generated beat-map against the user-created beat-map. Figure 4 has a visual graph of our training results along with validation. The training accuracy average is 88 percent, and the testing accuracy average is 92 percent. This model doesn't perform as well as the second network as you'll see later on.\n",
    "% TRAINING SECOND NEURAL NETWORK\n",
    "\\subsection{Second Neural Network Result}\n",
    "\\begin{figure}[htpb]\n",
    "    \\centering\n",
    "    \\includegraphics[width=\\textwidth,height=6cm,keepaspectratio=true]{cnn_model.png}\n",
    "    \\caption{\n",
    "        The training and testing results of our second neural network.\n",
    "    }\n",
    "    \\label{fig:Second Neural Network Training Result}\n",
    "\\end{figure}\n",
    "This network trains and validates in a similar way to our first neural network. The main difference being that this network removes the LSTM layer and instead handles the time aspect entirely through the CNN layers. Figure 5 shows our training results with validation. As you can see, training and validation accuracy is higher than the first network. Both the average training and training accuracy is 98 percent. So the initial attempt to input a song and its corresponding beatmap in order to generate a similar beatmap has been successful. Based on this, we can verify that we have a well-trained network that is capable of reproducing a valid beat-map."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "Both of the resulting networks show promise. Each of the networks have high training and validation accuracy towards generating a similar beat-map for the given song. However, this may not be indicative of a well-formed beat-map. Further testing needs to be done in order to verify that the beat-maps generated by the neural net are generalizing well and are not simply memorizing the input data. We hope that given enough songs and their corresponding beat-maps, we will be able to train a network that is able to successfully generalize and be able to generate new beat maps for any song. While we were not able to meet this goal thus far, we can conclude that we are moving in the right direction. Our future considerations would consist of specifying song genre when inputting songs and their beat-map into the neural network as well as trying to generate maps for the varying difficulties in Beat Saber."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
